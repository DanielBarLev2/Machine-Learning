from forward import l_model_forward, cost_forward
from sklearn.model_selection import KFold
from keras.utils import to_categorical
from backward import l_model_backward
from matplotlib import pyplot as plt
from keras.datasets import mnist
import numpy as np


class NeuralNetwork:
    def __init__(self, x_input: np.ndarray, y_label: np.ndarray, layer_dims: np.ndarray,
                 learning_rate: float, epoch: int, batch_size: int):

        self.x_input = x_input
        self.y_label = y_label
        self.layer_dims = layer_dims
        self.learning_rate = learning_rate
        self.epoch = epoch
        self.batch_size = batch_size


    def evaluate(self, n_folds=5):
        """

        """
        parameters = []

        k_fold = KFold(n_folds, shuffle=True, random_state=1)

        # divides to folds
        for x_train_i, x_test_i in k_fold.split(self.x_input):

            x_train_fold, y_train_fold, x_test_fold, y_test_fold = \
                self.x_input[x_train_i], self.y_label[x_train_i], self.x_input[x_test_i], self.y_label[x_test_i]

            parameters = self.train(x_train=x_train_fold, y_train=y_train_fold)

            if input("do you want to test the model on validation data? y/n:").__eq__("y"):
                accuracy = self.predict(x_input=x_test_fold, y_label=y_test_fold, parameters=parameters)
                print(f'model accuracy is: {100 * accuracy}%')

            if input("continue to next fold?").__eq__("y"):
                pass
            else:
                break

        print("done")

    def train(self, x_train: np.ndarray, y_train: np.ndarray) -> dict:
        """
        Description: Implements L-layer neural network. All layers but the last should have the ReLU
        activation function, and the final layer will apply the softmax activation function.
        The size of the output layer should be equal to the number of labels in the data.

        Input:
        x_input – the input data, a numpy array of shape (height*width , number_of_examples).
        y_train – the “real” labels of the data, a vector of shape (num_of_classes, number of examples).
        Layer_dims – a list containing the dimensions of each layer, including the input.
        batch_size – the number of examples in a single training batch.
        learning_rate - by how amount to change the weight matrices.
        epoch - the learning cap or limit.

        Output:
        parameters – the parameters learnt by the system during the training.
        costs – the values of the cost function (calculated by the compute_cost function).
        One value is to be saved after each 100 training iteration.
        """

        x_train = self._prepare_x_inputs(x_input=x_train)
        y_train = self._prepare_y_labels(y_label=y_train)
        parameters = self.initialize_parameters()

        cost_list = []

        # divides to batches
        x_train_batch, y_train_batch, num_batches = \
            self._create_batches(x_data=x_train, y_data=y_train, batch_size=self.batch_size)

        batch_index = 0

        for i in range(self.epoch):

            # return to first batch when finished
            if batch_index == num_batches:
                batch_index = 0

            last_activation, caches = l_model_forward(x_input=x_train_batch[batch_index], parameters=parameters)

            cost = cost_forward(last_activation=last_activation, y_train=y_train_batch[batch_index])

            gradients = l_model_backward(last_activation=last_activation, y_train=y_train_batch[batch_index],
                                         caches=caches)

            parameters = self.update_parameters(parameters=parameters, gradients=gradients)

            batch_index += 1

            cost_list.append(cost)
            if i % 1000 == 0 and i != 0:
                print(f'The cost after {i} iterations is: {cost}')

        plt.plot(cost_list)
        plt.show()

        return parameters


    def initialize_parameters(self):
        """
        Description: initialize weight matrices with random values and bias vectors with zeros.
        layer_dims: an array of the dimensions of each layer in the network.

        output: parameters - a dictionary containing the initialized w and b parameters.
        """
        parameters = {}

        for layer in range(1, len(self.layer_dims)):
            parameters[f'W{layer}'] = np.random.randn(self.layer_dims[layer], self.layer_dims[layer - 1]) * 0.1
            parameters[f'b{layer}'] = np.zeros((self.layer_dims[layer], 1))

        return parameters


    def update_parameters(self, parameters: dict, gradients: dict) -> dict:
        """
        Description: Updates parameters using gradient descent

        Input:
        parameters – a python dictionary containing the DNN architecture’s parameters
        gradients – a python dictionary containing the gradients (generated by L_model_backward)
        learning_rate – the learning rate used to update the parameters (the “alpha”)

        Output:
        parameters – the updated values of the parameters object provided as input
        """

        l = len(parameters) // 2

        for l in range(1, l + 1):
            parameters[f'W{l}'] = parameters[f'W{l}'] - self.learning_rate * gradients[f'dW{l}']
            parameters[f'b{l}'] = parameters[f'b{l}'] - self.learning_rate * gradients[f'db{l}']

        return parameters


    def predict(self, x_input: np.ndarray, y_label: np.ndarray, parameters: dict) -> float:
        """

        """
        x_input = self._prepare_x_inputs(x_input=x_input)
        probs, _ = l_model_forward(x_input=x_input, parameters=parameters)
        predictions = np.argmax(probs, axis=0)
        accuracy = (predictions == y_label).sum() / len(y_label)

        return accuracy

    @staticmethod
    def _prepare_x_inputs(x_input: np.ndarray) -> np.ndarray:
        """
        Description: prepare inputs by vectorization, transposing and normalizing input data.
        return: input in shape [num of features in [0,1], num of samples]
        """
        x_input =  x_input.reshape((x_input.shape[0], 28 * 28))
        x_input =  x_input.T
        x_input =  x_input / 255

        return x_input


    @staticmethod
    def _prepare_y_labels(y_label: np.ndarray) -> np.ndarray:
        """
        Description: one-hot encoding
        return: output labels in [ground truth, num of samples]
        """
        y_label = to_categorical(y_label, num_classes=10)
        y_label = y_label.T

        return  y_label

    @staticmethod
    def _create_batches(x_data: np.ndarray, y_data: np.ndarray, batch_size: int) -> tuple[list, list, int]:
        num_samples = x_data.shape[1]
        num_batches = num_samples // batch_size
        batches_x = []
        batches_y = []

        # Create full-sized batches
        for i in range(num_batches):
            batch_x = x_data[:, i * batch_size: (i + 1) * batch_size]
            batch_y = y_data[:, i * batch_size: (i + 1) * batch_size]
            batches_x.append(batch_x)
            batches_y.append(batch_y)

        # Create the last smaller batch, if necessary
        if num_samples % batch_size != 0:
            batch_x = x_data[:, num_batches * batch_size:]
            batch_y = y_data[:, num_batches * batch_size:]
            batches_x.append(batch_x)
            batches_y.append(batch_y)

        return batches_x, batches_y, num_batches
