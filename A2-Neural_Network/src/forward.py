import activation_functions
import numpy as np

def initialize_parameters(layer_dims: np.ndarray) -> dict:
    """
    Description: initialize weight matrices with random values and bias vectors with zeros.

    input: an array of the dimensions of each layer in the network.
    (layer 0 is the size of the flattened input, layer L is the output sigmoid)

    output: parameters - a dictionary containing the initialized w and b parameters of each layer (W1…WL, b1…bL).
    """

    parameters = {}

    for layer in range(1, len(layer_dims)):
        parameters[f'W{layer}'] = np.random.randn(layer_dims[layer], layer_dims[layer - 1]) * 0.1
        parameters[f'b{layer}'] = np.zeros((layer_dims[layer], 1))

    return parameters


def linear_forward(activation, w: np.array, b: np.array, layer: int):
    """
    Description: Implement the linear part of a layer's forward propagation.

    input:
    activation – the activations of the previous layer.
    w – the weight matrix of the current layer (of shape [size of current layer, size of previous layer]).
    B – the bias vector of the current layer (of shape [size of current layer, 1]).

    Output:
    Z – the linear component of the activations function (i.e., the value before applying the non-linear function).
    cache – a dictionary containing activation, w, b (stored for making the backpropagation easier to compute).
    """

    cache = {}
    z = np.dot(w, activation) + b
    cache[f'x{layer}'] = activation
    cache[f'W{layer}'] = w
    cache[f'b{layer}'] = b

    return z, cache


def linear_activation_forward(prev_activation: np.ndarray, w: np.ndarray, b: np.ndarray, activation_function: str,
                              layer: int) -> tuple[np.ndarray, dict]:
    """
    Description: Implement the forward propagation for the activation function decision.

    Input:
    prev_activation – activation of the previous layer.
    w – the weights matrix of the current layer.
    B – the bias vector of the current layer.
    activation_function – the activation function to be used (either “sigmoid” or “relu”).

    Output:
    activation – the activation of the current layer
    cache – a joint dictionary containing both cache and activation_cache
    """

    # calculate the linear part.
    z, cache = linear_forward(activation=prev_activation, w=w, b=b, layer=layer)

    if activation_function.__eq__("softmax"):
        activation, activation_cache = activation_functions.softmax(z=z)
        cache[f'z{layer}'] = activation_cache

        return activation, cache

    elif activation_function.__eq__("relu"):
        activation, activation_cache = activation_functions.relu(z=z)
        cache['z'] = activation_cache

        return activation, cache


def l_model_forward(x_data: np.ndarray, parameters: dict) -> tuple[np.ndarray, list[dict]]:
    """
    Description: Implement forward propagation for the entire network computation.
    [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX

    Input:
    x_data – the data.
    numpy array of shape (input size, number of examples)
    parameters – the initialized W and b parameters of each layer

    Output:
    last_activation – the last post-activation value
    caches – a list of all the cache objects generated by the linear_forward function
    """

    activation = x_data
    caches = []
    last_layer = 0

    for layer in range(1, (len(parameters) // 2)):
        prev_activation = activation

        # apply the linear activation using relu
        activation, cache = linear_activation_forward(prev_activation=prev_activation, w=parameters[f'W{layer}'],
                                                  b=parameters[f'b{layer}'], activation_function="relu", layer=layer)

        caches.append(cache)

        last_layer = layer + 1

    # at the last layer, activates the softmax layer
    last_activation, cache = linear_activation_forward(prev_activation=activation, w=parameters[f'W{last_layer}'],
                                                       b=parameters[f'b{last_layer}'], activation_function="softmax",
                                                       layer = last_layer)

    caches.append(cache)

    return last_activation, caches


def cross_entropy_loss(last_activation: np.ndarray, y_train: np.array) -> float:
    """
    Description: Implement the cost function defined by equation.
    The requested cost function is categorical cross-entropy loss.

    Input:
    last_activation – probability vector corresponding to your label predictions
    shape (num_of_classes, number of examples).
    y_train – the true labels vector. encoded as one-hot.
    shape (num_of_classes, number of examples).

    Output:
    cost – the cross-entropy cost
    """

    m = last_activation.shape[1]

    cost = (1 / m) * np.sum((- np.log( np.exp(last_activation) / np.sum(np.exp(y_train)))))

    return cost