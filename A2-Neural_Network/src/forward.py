import numpy as np
import activation_functions

def initialize_parameters(layer_dims: np.ndarray) -> dict:
    """
    input: an array of the dimensions of each layer in the network.
    (layer 0 is the size of the flattened input, layer L is the output sigmoid)

    output: a dictionary containing the initialized w and b parameters of each layer
    (W1…WL, b1…bL).
    """
    np.random.seed(99)

    parameters = {}

    for layer in range(1, len(layer_dims)):
        parameters[f'W{layer}'] = np.random.randn(layer_dims[layer], layer_dims[layer - 1]) * 0.1
        parameters[f'b{layer}'] = np.zeros((layer_dims[layer], 1))

    return parameters


def linear_forward(activation, w: np.array, b: np.array):
    """
    Description: Implement the linear part of a layer's forward propagation

    input:
    activation – the activations of the previous layer
    w – the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
    B – the bias vector of the current layer (of shape [size of current layer, 1])

    Output:
    Z – the linear component of the activations function (i.e., the value before applying the non-linear function)
    cache – a dictionary containing activation, w, b (stored for making the backpropagation easier to compute)
    """

    z = np.dot(w, activation) + b
    cache = (activation, w, b)

    return z, cache


def linear_activation_forward(prev_activation: np.ndarray, w: np.ndarray, b: np.ndarray, activation_function: str) ->\
        tuple[np.ndarray, tuple]:
    """
    Description: Implement the forward propagation for the LINEAR -> ACTIVATION layer

    Input:
    prev_activation – activation of the previous layer
    w – the weights matrix of the current layer
    B – the bias vector of the current layer
    activation_function – the activation function to be used (a string, either “sigmoid” or “relu”)

    Output:
    activation – the activation of the current layer
    cache – a joint dictionary containing both linear_cache and activation_cache
    """

    z, linear_cache = linear_forward(activation=prev_activation, w=w, b=b)

    if activation_function.__eq__("softmax"):
        activation, activation_cache = activation_functions.softmax(z=z)

        cache = (linear_cache, activation_cache)

        return activation, cache

    elif activation_function.__eq__("relu"):
        activation, activation_cache = activation_functions.relu(z=z)

        cache = (linear_cache, activation_cache)

        return activation, cache


def l_model_forward(x_train: np.ndarray, parameters: dict) -> tuple[np.ndarray, list[tuple]]:
    """
    Description: Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation

    Input:
    x_train – the data, numpy array of shape (input size, number of examples)
    parameters – the initialized W and b parameters of each layer

    Output:
    last_activation – the last post-activation value
    caches – a list of all the cache objects generated by the linear_forward function
    """

    activation = x_train
    caches = []
    last_layer = None

    for layer in range(1, (len(parameters) // 2)):
        # sets prev activation and step to next activation
        prev_activation = activation

        # apply the linear activation using relu
        activation, cache = linear_activation_forward(prev_activation=prev_activation, w=parameters[f'W{layer}'],
                                                  b=parameters[f'b{layer}'], activation_function="relu")

        caches.append(cache)

        last_layer = layer + 1

    # at the last layer, activates the sigmoid layer
    last_activation, cache = linear_activation_forward(prev_activation=activation, w=parameters[f'W{last_layer}'],
                                                      b=parameters[f'b{last_layer}'], activation_function="softmax")

    caches.append(cache)

    return last_activation, caches


def compute_cost(last_activation: np.ndarray, y_train: np.array) -> float:
    """
    Description: Implement the cost function defined by equation. The requested cost function is categorical
    cross-entropy loss

    Input:
    last_activation – probability vector corresponding to your label predictions, shape (num_of_classes,
    number of examples)
    y_train – the labels vector (i.e. the ground truth)

    Output:
    cost – the cross-entropy cost
    """

    m = last_activation.shape[1]

    # cross-entropy loss calculation:
    cost = -1 / m * np.sum(y_train * np.log(last_activation) + (1 - y_train) * np.log(1 - last_activation))

    return cost