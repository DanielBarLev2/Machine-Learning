import activation_functions
import numpy as np


def linear_forward(activation, w: np.array, b: np.array, layer: int):
    """
    Description: Implement the linear part of a layer's forward propagation.

    input:
    activation – the activations of the previous layer.
    w – the weight matrix of the current layer (of shape [size of current layer, size of previous layer]).
    B – the bias vector of the current layer (of shape [size of current layer, 1]).

    Output:
    Z – the linear component of the activations function (i.e., the value before applying the non-linear function).
    cache – a dictionary containing activation, w, b (stored for making the backpropagation easier to compute).
    """
    cache = {}
    z = np.dot(w, activation) + b
    cache[f'x{layer}'] = activation
    cache[f'W{layer}'] = w
    cache[f'b{layer}'] = b

    return z, cache


def linear_activation_forward(prev_activation: np.ndarray, w: np.ndarray, b: np.ndarray, activation_function: str,
                              layer: int) -> tuple[np.ndarray, dict]:
    """
    Description: Implement the forward propagation for the activation function decision.

    Input:
    prev_activation – activation of the previous layer.
    w – the weights matrix of the current layer.
    B – the bias vector of the current layer.
    activation_function – the activation function to be used (either “sigmoid” or “relu”).

    Output:
    activation – the activation of the current layer
    cache – a joint dictionary containing both cache and activation_cache
    """
    # calculate the linear part.
    z, cache = linear_forward(activation=prev_activation, w=w, b=b, layer=layer)

    if activation_function.__eq__("softmax"):
        activation, activation_cache = activation_functions.softmax(z=z)
        cache[f'z{layer}'] = activation_cache

        return activation, cache

    elif activation_function.__eq__("relu"):
        activation, activation_cache = activation_functions.relu(z=z)
        cache[f'z{layer}'] = activation_cache

        return activation, cache

    else:
        raise Exception('Non-supported activation function')


def l_model_forward(x_input: np.ndarray, parameters: dict) -> tuple[np.ndarray, list[dict]]:
    """
    Description: Implement forward propagation for the entire network computation.
    [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX

    Input:
    x_input – the data.
    numpy array of shape (input size, number of examples)
    parameters – the initialized W and b parameters of each layer

    Output:
    last_activation – the last post-activation value
    caches – a list of all the cache objects generated by the linear_forward function
    """
    activation = x_input
    caches = []

    for layer in range(1, (len(parameters) // 2)):
        prev_activation = activation

        # apply the linear activation using relu
        activation, cache = linear_activation_forward(prev_activation=prev_activation, w=parameters[f'W{layer}'],
                                                  b=parameters[f'b{layer}'], activation_function="relu", layer=layer)

        caches.append(cache)

    last_layer = (len(parameters) // 2)
    # at the last layer, activates the softmax layer
    last_activation, cache = linear_activation_forward(prev_activation=activation, w=parameters[f'W{last_layer}'],
                                                       b=parameters[f'b{last_layer}'], activation_function="softmax",
                                                       layer = last_layer)

    caches.append(cache)

    return last_activation, caches


def cost_forward(last_activation: np.ndarray, y_train: np.array) -> float:
    """
    Description: Implement the cost function defined by equation.
    The requested cost function is categorical cross-entropy loss.

    Input:
    last_activation – probability vector corresponding to your label predictions
    shape (num_of_classes, number of examples).
    y_train – the true labels vector. encoded as one-hot.
    shape (num_of_classes, number of examples).

    Output:
    cost – the cross-entropy cost
    """
    m = last_activation.shape[1]

    cost = - (1 / m) * np.sum(y_train * np.log(last_activation))

    return cost